# 데이터 분석 입문(From Fastcampus)



### 데이터 가치

- 데이터는 금광
- 데이터는 새로운 원유
- 크기로는 가치 판단은 `불가`
- **활용 가능성 및 파급효과** 가 중요



### How to make value?



Example

* 금광에서 가치 만드는 과정
  * 지질 조사 -> 채굴 설비 -> 금 채굴 -> 가공 -> `상품`



### 데이터 분석의 가치



* 불확실한 미래 대비
  * 데이터 속에 담긴 Insight 확인
  * 아무것도 결정되지 않은 미래 예측 가능
* 데이터 기반 의사결정
  * 직관이 아닌 객관적 데이터 분석을 활용
* 새로운 소통 언어
  * 새 시대 소통의 언어
  * 데이터를 모르면, 회의에서 이야기 자체를 하기 어려움
  * 다만, 데이터 분석은 매우 큰 카데고리



### 데이터 분석 어떻게 하는 거죠?



* 데이터 분석 절차 및 방법 -> `정답`이 없음
* 분석가가 데이터 분석 방법 결정 시 아래 요소 참고
  * **분석 목표, 목적 **
  * **데이터 종류와 특성**



ex. 연구 및 비즈니스 데이터 분석

* 연구 및 데이터 분석
  * 실험, 설문을 통한 데이터 수집

* 비즈니스 데이터 분석
  * 별도의 데이터 수집 절차 없이 기업활동에 쌓인 데이터 즉시 활용 가능
  * 수집 절차가 중요하지는 않음
  * 다만, 적절한 분석 목표 세우는 것이 hard
* 경계 모호 데이터 분석
  * ex. 국민건강보험공단 진료내역 데이터를 통한 질환 발병 연구



**데이터 분석 주요 과정**

* 먼저 `데이터 수집`
* 분석 목적에 맞게 `데이터 가공`
* 적절한 방법으로 `데이터 분석`
* 분석 결과, `시각화 및 문서화` -> 누군가에게 분석 결과를 공유해야 함



### How to 데이터 수집?



활용 가능 데이터

* 내부 데이터(ex. 사내 DB, 기존 연구 데이터)
* 직접 수집 데이터(ex. 실험 결과 및 설문/리서치 결과)
* 외부 데이터(ex. 정부 기관이 보유한 `공공 데이터`, 일부 업체가 공개한 `민간 데이터`)
* 일반적으로 우리의 내부 데이터에 외부 데이터를 appitizer 로 쓰는 개념



외부 데이터 활용 예제

* 커피 전문점 데이터 + 날씨 데이터



공공 데이터 활용

* [공공데이터포털](data.go.kr)
* [통계청 MDIS](mdis.kostat.go.kr)
  * 통계조사 원본 데이터 제공(ex. 경제총조사, 인구총조사 등)
* [서울시 열린데이터광장](data.seoul.go.kr)



민간 데이터 활용

* 일부 기업이 제한적 데이터로 공개
* [SKT 빅데이터허브](bigdatahub.co.kr)
* [네이버 데이터랩](datalab.naver.com)
* [Kaggle](kaggle.com)



### How to 데이터 가공?

* 확증적 분석
  * confirmatory data analysis
    * 미리 설정한 가설을 확인하기 위한 분석
    * 추정(estimation) 및 검정(test) 활용
* `탐색적 분석`
  * exploratory data analysis
    * 변수, 변수 관계 데이터 자체 특성 확인하기 위한 분석
    * 간단한 기술 통계량 계산과 다양한 그래프 활용
    * 모든 데이터 분석 시작에서의 필수



**데이터 가공 방법**

* 요약(aggregation)

  * `데이터 정보를 인식 가능한 수준으로 줄이는 과정`
  * 그룹별 관측 수, 평균, 최댓값 계산 등 단순 숫자 요약
    * ex. 매장별 혼잡 시간 계산

  

* 모형(model) -> 더 복잡한 가공 방법

  * 정해진 알고리즘에 따라 데이터 속 변수와 관측치 관계 확인
  * 가능성을 수치화환 확률로 설명
    * ex. 날씨,요일,시간대 별 매장 손님수와 주문상품 예측
  * 요약 보다는 조금 더 복잡(알고리즘이 추가되기 때문)



### 데이터 가공의 필요성

* 데이터 가공(manipulation)
  * 데이터 인식 및 분석을 위해 데이터 형태를 바꾸는 과정
* 1st. 부분 데이터 선택
  * 관심있는 관측치 및 변수 선택
* 2nd. 변수 결합, 분해 및 파생변수 생성
  * 기존 변수를 더 활용하기 좋은 형태로 변환
  * ex. 고객의 실제 연령 대신 연령대로 선택



### 데이터 분석 시작

분석 실행

* 분석 목표 설정
  * 실행 및 활용 가능성 고려
* 데이터 선택 및 수집
  * 내부 or 외부 데이터
  * `동일한 분석을 반복`하여 결과 재현 확인
* 탐색적 데이터 분석
  * 변수 및 변수 관계 분석
* 확증적 데이터 분석 및 모형 적합
  * 검정, 알고리즘 등 활용한 분석 실행
* 적용
* 분석 결과, 시각화 및 문서화 하기



**직접 결정을 못하지만**, 시각화 문서화를 잘하여 의사결정권자에게 자료를 잘 전달하는 것이 중요



**분석 결과 공유**

* 전체 분석 과정이 아닌, 분석의 흐름을 이해 가능한 수준으로 요약
* 효과적 정보 전달을 위한 `그래프 활용`
* 적절한 도구 활용하여 전달
  * ex. MS OFFICE(Excel, Word, PowerPoint)
  * markdown : R, Python 등에서 분석과 동시에 보고서 작성 가능
  * dashboard : 웹 기반 동적 보고서 작성 가능
    * ex. R의 Shiny를 활용한 대시보드



# 나도 데이터 분석 가능?



### 데이터 분석가 역할

보통은 `분석 목적 및 방향 결정` 부터 `모델 생성 및 평가` 까지 역할

* 분석 목적 및 방향 결정
  * 인터뷰 및 현황 분석
  * 분석 주제 발굴
  * 분석 방향 정의
* 데이터 선택 및 수집
  * 가설 수집
  * 분석 데이터 정의
  * 분석 데이터 수집
* 데이터 탐색 및 정제(**여기까지가 모델 생성하는 과정**)
  * 샘플링 및 데이터 정제
  * 파생 변수 생성
  * 데이터 탐색
* 모델 생성 및 평가
  * 데이터 모델링
  * 결과 검증
  * 모델 평가
* 적용(여기부터는 `데이터 엔지니어` 역할)
  * 프로세스 설계
  * 시스템 개발
  * 운영환경 적용 및 모니터링



### 데이터 분석가

* 역량
  * 통계 지식
  * 수학
  * R, 파이썬, 엑셀 등 분석 툴 활용
* 비즈니스 지식
* 커뮤니케이션 역량
  * 문서 작성 스킬
  * 시각화



**데이터 분석**이 왜 트렌드?

* 사실 과거에도 이 주제가 화두였으나, 단일 컴퓨팅에 머무르며 계속 발전하지는 못함

* Hadoop, Spark 등 분산 컴퓨팅이 발전하면서, 여러 가치를 생성할 수 있기 때문
* 예를 들어, 제조업에서 불량품을 찾아내는 작업, 마케팅에서는 개인화된 추천



### 데이터 엔지니어

* 이전 5단계 중 `적용` 단계에서 가장 큰 역할

* 역할(`수집부터 시각화`까지 데이터 파이프라인의 성능 최적화 역할)

* | Process               | Task                                                         | 기술요소                         |
  | --------------------- | ------------------------------------------------------------ | -------------------------------- |
  | 데이터 수집           | 원천 데이터 확인, 수집 주기 및 방법 결정, 인프라 결정        | Kafka, Flume, Cloud Infra        |
  | 데이터 통합 및 전처리 | 데이터 통합 및 저장, 데이터 전처리, 데이터 예외처리 방안 결정 | RDB, NoSQL, MapReduce, Spark     |
  | 분석 알고리즘         | 개발언어, 프로세스 설계, 분석 알고리즘 개발                  | Java, Scala, R, Python, Spark ML |
  | 시각화                | Chart 툴 도입, Report 생성, 결과 검증                        | Chart, 시각화, Kibana, Zeppelin  |
  | 운영                  | 시스템 모니터링, 데이터 정합성 모니터링                      | 모니터링 툴                      |



* 주요 개발 역량
  * 컴퓨터 엔지니어링
  * 개발 언어
  * 데이터 저장 기술
  * Web 시스템화 기술

* 빅데이터 인프라 역량
  * 데이터 수집 기술
  * 분산 파일 시스템, 분산 컴퓨팅
  * 클라우드 기술



### 데이터 사이언티스트

* **그냥 통계학자를 멋있게 부르는 이름** - Nate Silver
* **복잡한 문제 해결하는 기술적 능력 및 호기심 있는 새로운 유형 데이터 분석 전문가** - sas



왜 갑자기 데이터 사이언티스트가 뜨게 되었을까?



과거

* 과거에는 R, SAS, SPSS, 엑셀 등 사용 -> 단일 Machine 분석에 한계
* 정형 데이터 -> Row, Column 형태
* 일반 통계 기법



TO-BE

* Hadoop, Spark 등 분산 컴퓨팅 기반 빅데이터 기술 등장
* Image, Text, Json, XML 등 비정형 데이터도 분석 가능
* 기계학습 및 딥러닝



분석 행위는 동일하지만, SW 활용 능력 + 머신러닝 + AI 기법 까지 알고 있어야 데이터 사이언티스트가 될 수 있음



주요 역량

* 비즈니스 지식
* 통계학 및 기계 학습
* R, Python 등 프로그래밍 스킬
* Big Data 기술 이해
* 시각화 및 보고
* **호기심**



### 제조업 데이터 분석 예시

* 품질/수율 개선
  * 수율 : 생산품 대비 결함 없는 합격품 비율(높을수록 좋음)
* 생산/운영 최적화
* 장비/설비 관리, 정비
* 에너지, Utility 절감
* 안전, 보건, 환경
* 판매 예측, 재고 관리



최근에는 스마트팩토리를 통해 제조 전 영역에서 데이터 분석 적용

제조 데이터 특징으로 `변수 개수가 크다` -> **다중공선성** 문제

**다중공선성** : 독립변수들 간 높은 선형관계 존재 의미, 적절히 제거 필요

**불균형 자료** : 양품 vs 불량 비율 중 압도적으로 양품이 높음 -> 기계 학습 시에도 불균형 자료 비율 참고해야 됨



### 통신 마케팅

* 고객 상품 추천 분석 사례
  * 도서 구매 추천
  * 영화 추천
  * 추천 알고리즘(1)
    * **연관 규칙(Association Rule)** : 함께 구매가 발생하는 규칙
      * 각 상품을 미리 매칭 시, 하나의 상품 구매 시 다른 상품도 추천 목록에 뜨도록
  * 추천 알고리즘(2)
    * **협업 필터링(Collaborative Filtering)**
      * 사용자 기반 필터링(사용자 구매 데이터 참고)
      * 아이템 기반 필터링(상품 간 유사도)
      * **서로 간 유사 정보 수치화** 하는 것이 필수
  * A/B 테스트
    * 한 집단에는 기존 방법
    * 다른 집단은 새로운 방법
    * 어느 집단이 성과가 더 높은 지를 활용하여 정량적 평가 시도
* 상권 분석 사례
  * 통신업에서 중요한 데이터 -> `위치 기반 데이터`
  * **기지국 데이터** 및 **카드사용 이력 데이터** 기반
    * 다만, **개인정보 이슈**로 인하여 집단 단위로 분석하는 경우가 많음.



### 금융/보험



* 고객 이동 경로 분석 사례
  * 웹 페이지 행동을 통한 경로 패턴 파악 -> 마케팅 정보 제공
  * ex. SOL - 환율 조회하는 사람에게 여행 적금 권유
* 신용평가 사례
  * 우리은행 - 빅데이터 활용 기업 진단 시스템 구축. 200여개 리스크 지표로 기업 부실 가능성 4단계 파악
  * KBank - 통신요금 납부 실적, 신용카드 결제 정보 등 활용 자체 신용 평가 시스템 구축
  * Lenddo - 글로벌 핀테크 업체, 여러 비정형 데이터를 추출하여 신용도 평가
* 이상 금융 거래 탐지 사례
  * FDS(Fraud Detection System)
    * 거래 정보 합법적인 수집
    * 정상 사용 패턴 추출
    * 여러 사용 패턴 중, 정상 패턴과 벗어날 시 사전 차단하는 구조
* 보험 마케팅 고객 유치 사례
  * 운전 습관을 통한 보험료 측정
  * 건강 증진 프로그램
    * 체력 증진 프로그램으로 건강 생활 습관 유지 고객 대상, Point 적립 및 기타 혜택 제공
* 제약 사항
  * 고객정보 공유 규제
    * 계열사 간 공유 제약
    * 금융산업 빅데이터 활성화 주요 제약 허들
  * 고객 개인정보 유출 시 막대한 피해



### 헬스케어/제약

* 의료 빅데이터 및 AI
  * 개인별 맞춤 자료 제공 - 환자 진단정보, 의료차트, 간호기록, 유전체, 개인습관 데이터 활용
  * 치료 중심이 아닌 **예방** 에 초점
* AI 기반 질병판독
  * AI 를 통해 흉부 x선 영상에서 폐 결절 이상부위 검출
  * 의사 판독을 보조
  * 폐 결절 검출 정확도 97% 까지, 의사판독 최대 20% 향상
* (참고) 보건 의료 빅데이터 개방 시스템
  * 건강보험심사평가원 공공 데이터 개방
  * 공공 및 의료 빅데이터, 의료통계 분석 관련 정보 제공



### 해외 데이터 분석 사례

* 고객 맞춤 서비스 제공
* 프로세스 효율 개선
  * DHL - 일별 배송정보를 분석하여 소비자 이용흐름 패턴 분석
  * Google - 데이터센터 에너지 최적화, 데이터 센터 성능 및 에너지 사용량 균형 유지. 구글 딥마인드 활용한 데이터 냉각 전력 40% 감소
  * Amazon - 고객 장바구니, 반품 내역, 마우스 커서 머무른 시간 등 분석하여 미래 발생할 고객 근처 주소지로 미리 물건을 예측하여 배송(미국의 특수한 사례)



### 데이터 분석 툴 비교

* 주요 언어 및 도구

  * R
  * Python
  * 엑셀
  * Spark
  * Splunk
  * and so on

  |      | R                                                 | Python                                                       | Excel                                                        |
  | ---- | ------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 개요 | 통계 계산 및 그래픽 목적 오픈소스 프로그래밍 언어 |                                                              | MS 오피스 툴 중 하나                                         |
  | 장점 | 데이터 시각화, 넓은 생태계, 다양한 통계 함수      | 다양한 API, Tensorflow, 높은 생산성                          | GUI 방식, 빠른 보고서 작성 가능                              |
  | 단점 | 성능, 메모리 한계, 분석 이외 활용 제약            | R에 비해 부족한 통계 함수, 속도는 빠르지만 Java 혹은 C 에 비하면 성능에 한계 | 대용량 데이터 분석 한계, 정형 데이터만 분석 가능 타 시스템 연계 어려움 |

* When

  * R 
    * 통계 분석
    * 다양한 통계 라이브러리 활용
  * Python
    * 빠른 계산 성능
    * 타 시스템과 다양한 연계 필요
  * Excel
    * 프로그래밍이 어렵거나, 보고서 작성이 중요
    * 쉬운 조작
    * 빠른 보고서 작성 가능



### 알쓸데잡



**기술통계**

* 수집한 자료를 분석하여 대상 속성을 파악하는 분석 방법
* 중심 경향값 : 대표 수치값
  * 평균 : 전체 자료 총합 / 전체 자료 수
  * 중앙값 : 최대값, 최소값 정 가운데 수치
  * 최빈값 : 가장 많은 빈도를 가진 수치
  * 분산도 : 전체 자료가 얼마나 퍼져있는가
  * 분산 : 각 자료가 평균으로 부터 떨어진 거리, 편차들의 제곱 수치 총합 / 전체 자료
  * 상관계수 : 두 변수 간 관계 크기
  * 회귀계수 : 독립변수가 종속변수에 미치는 영향 크기

**추리통계**

* **모집단 대표 표본 추출**, 모집단 속성을 유추
* 신뢰 구간 : 모집단의 특성이 위치할 가능성이 높은 구간
  * 95%
  * 99%
  * 99.9%



**모집단**

* 연구 또는 분석이 이뤄지는 전체 대상
* 현실상, 모집단 전수 조사는 불가능

**표본**

* 모집단 추출 일부, 모집단 속성을 유추하는 데에 사용
* 확률 표본 추출 : **무작위 표본 추출**
* 비확률 표본 추출 : **조사자 편의 및 판단에 의해 표본 추출**
  * 확률 표본 추출에 비해 쉬운 방법
  * 표본을 얼마나 추출해야 모집단과 가까운지? -> 중심극한정리



**중심극한정리**

* 표본 30 이상 충분히 클때
* 모집단 분포 상관없이 표본은 정규분포
* 표본 평균 = 모집단 평균
* 표본 분산 = (모집단 분산)/(표본 수)



**자유도**

* 평균 유지하면서 자유롭게 어떤 값도 가질 수 있는 사례 수(N-1)
* N : 표본의 수



